# Video Walkthrough Guide - Complete Script

## Overview
This is a complete, detailed script for a 5-10 minute video walkthrough. You can read directly from this document while recording.

## ğŸ¯ BRIEF SUMMARY - What to Say

**Key Features to Highlight:**
1. **Streaming Responses**: Answers stream token-by-token in real-time (like ChatGPT)
2. **Stop/Regenerate**: Users can cancel generation or regenerate responses
3. **Source Citations**: Clickable source links with document names and URLs
4. **Trash/Recovery**: 30-day recovery window for deleted items
5. **Multi-Modal**: PDFs, audio (Whisper), images (OpenAI Vision), URLs, text
6. **Hybrid Retrieval**: Combines semantic search (85%), keyword matching (15%), and recency (10%)
7. **Temporal Queries**: Natural language time filtering ("last week", "yesterday")
8. **Fast Processing**: Batch embedding generation (10-100x faster than sequential)
9. **Architecture**: Microservices with React frontend, Node.js backend, Chroma vector DB, Redis queue

---

## FULL SCRIPT

### SECTION 1: INTRODUCTION (30-45 seconds)

**Read this:**

"Hello! My name is [Your Name], and welcome to this walkthrough of SecondBrain - a multi-modal personal knowledge management system that I've built. In this video, I'll demonstrate how the application works, and more importantly, I'll walk you through the key architectural decisions I made and the trade-offs involved in each choice. This will give you insight into not just what the system does, but why it's built the way it is. Let's get started!"

---

### SECTION 2: APPLICATION DEMONSTRATION (3-4 minutes)

#### Part A: Uploading Different Content Types

**Read this:**

"First, let me show you how SecondBrain handles different types of content. The system is truly multi-modal - it can ingest PDFs, audio files, images, web URLs, and plain text. Let me demonstrate each one."

**Now demonstrate:**

"Here, I'm uploading a PDF document. I'll drag and drop it into the upload area. Notice how the system immediately acknowledges the upload and shows a success message. Behind the scenes, the file is being processed asynchronously - the backend stores the metadata and enqueues a job to a Redis queue. A worker will process this job in the background, extracting text from the PDF, chunking it, and generating embeddings in batches. The system uses batch embedding generation, processing up to 100 chunks at once, which makes it 10-100 times faster than processing them one by one. This async approach with batch processing means users get immediate feedback and documents are searchable within seconds, even for large files."

**Continue:**

"Now let me upload an image. I'll select this PNG file. For images, the system uses OpenAI's Vision API with GPT-4o-mini to generate descriptive captions. The caption becomes the text that gets embedded and indexed. This allows you to search for images using natural language queries about their content, not just filenames."

**Continue:**

"Next, I'll paste some plain text directly. This is useful for quick notes or snippets of information. The text is ingested immediately and processed the same way as other content types."

**Continue:**

"Finally, let me show you URL ingestion. I'll enter a web URL here. The system will scrape the webpage, extract the main content, and index it. This is particularly useful for saving articles, blog posts, or any web content you want to remember and query later."

#### Part B: Querying the Knowledge Base

**Read this:**

"Now let's see how the system answers questions. The real power of SecondBrain is in its ability to understand natural language queries and retrieve relevant information from your knowledge base."

**Demonstrate:**

"I'll ask: 'What did I learn about [topic from your uploaded content]?' Watch how the system processes this query. Notice the answer streams in token-by-token, just like ChatGPT - this provides immediate feedback and a more natural conversation experience. Behind the scenes, the system generates an embedding of the query using OpenAI's text-embedding-3-small model, then searches the vector database for semantically similar chunks. The system uses a hybrid retrieval approach that combines semantic similarity with keyword matching and recency scoring. The top results are sent to GPT-4o-mini as context, and the LLM streams the answer based only on the retrieved information. Notice the source citations at the bottom - these are clickable links that show which documents were used. If multiple chunks come from the same document, it's only shown once to keep the sources clean."

**Continue:**

"Let me try a temporal query: 'What did I upload last week?' The system uses chrono-node, a natural language date parser, to understand time expressions like 'last week', 'yesterday', or 'in December'. It filters the candidate chunks to only those within the specified time window before scoring them. This makes time-based queries much more accurate and efficient."

**Continue:**

"Here's a more complex query: [ask a multi-part question]. Notice how the system retrieves information from multiple documents and synthesizes an answer. The hybrid scoring ensures we get both semantically relevant and keyword-matched results, giving us comprehensive coverage of the topic."

#### Part C: Document Management

**Read this:**

"Let me show you the document management features. Here's the list of all uploaded documents. You can see each document's type - whether it's a PDF, image, audio file, text, or URL. The status indicator shows whether processing is complete. Green checkmarks mean the document has been fully processed and is searchable."

**Demonstrate:**

"I can search through my documents using this search bar. Let me filter by type - I'll show only PDFs. The system also displays the actual filename or title, making it easy to identify documents. Earlier versions showed internal paths or IDs, but I improved this to show user-friendly names."

**Continue:**

"Let me delete a document. When I click delete, a confirmation modal appears. This prevents accidental deletions. When confirmed, the document is moved to trash - it's not permanently deleted yet. This gives you a 30-day window to recover it if you change your mind."

**Continue:**

"Now let me show you the trash feature. I'll navigate to the Trash tab. Here you can see all deleted items - both documents and conversations. Each item shows when it was deleted and how many days until permanent deletion. I can restore any item with one click, or permanently delete it if I'm sure. Items older than 30 days are automatically removed to save space."

#### Part D: Streaming Features

**Read this:**

"Let me highlight some key features of the chat interface. Notice how responses stream in real-time - you can see the AI thinking as it types. If the response is taking too long or going in the wrong direction, you can click the Stop button to cancel generation. There's also a Regenerate button to get a fresh response to the same query. Each message has a timestamp showing when it was sent, making it easy to track conversation history."

**Demonstrate:**

"Watch as I ask a question - see how the text appears word by word? This streaming approach makes the interface feel more responsive and conversational. If I want to stop it, I just click Stop. And if I want a different answer, I can click Regenerate. The input field is disabled while the AI is generating to prevent sending multiple queries at once."

**Continue:**

"Notice the source citations below each answer. These show which documents were referenced. If I click on a source, it opens the original document or URL in a new tab. The system intelligently deduplicates sources - if multiple chunks from the same document are used, it only shows the document once with the highest relevance score."

#### Part E: Chat History

**Read this:**

"The system maintains conversation history, so you can revisit past queries and continue conversations. Here's the conversation sidebar. I can see all my past conversations, search through them, and load any one to see the full message history."

**Demonstrate:**

"Let me load a previous conversation. Notice how all the messages are preserved. I can continue this conversation or start a new one. Each conversation has a title that's automatically generated or can be set manually. This makes it easy to organize and find past discussions."

**Continue:**

"When I start a new conversation, it creates a fresh context. The system saves conversations to the backend, so they persist across sessions. I can also delete conversations I no longer need."

#### Part F: Dark Mode

**Read this:**

"One final feature I want to show is the dark mode toggle. The system supports both light and dark themes, and your preference is saved in localStorage. The theme persists across sessions and provides a better experience for different lighting conditions or personal preferences."

---

### SECTION 3: ARCHITECTURE WALKTHROUGH (4-5 minutes)

#### Part A: High-Level Architecture

**Read this:**

"Now let me walk you through the architecture. Understanding the system design will help you appreciate the decisions I made and why certain trade-offs were necessary."

**Show this diagram:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         CLIENT LAYER                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  React Frontend (Port 5173)                               â”‚   â”‚
â”‚  â”‚  - Chat Interface                                         â”‚   â”‚
â”‚  â”‚  - Document Upload                                        â”‚   â”‚
â”‚  â”‚  - Document Management                                    â”‚   â”‚
â”‚  â”‚  - Dark Mode Support                                      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â”‚ HTTP/REST
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      APPLICATION LAYER                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  Backend API         â”‚         â”‚  Worker Service      â”‚     â”‚
â”‚  â”‚  (Port 4000)         â”‚â—„â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  (Background Jobs)   â”‚     â”‚
â”‚  â”‚  - Express.js         â”‚  Redis  â”‚  - BullMQ Consumer   â”‚     â”‚
â”‚  â”‚  - REST Endpoints     â”‚  Queue â”‚  - Embedding Gen     â”‚     â”‚
â”‚  â”‚  - Query Processing   â”‚         â”‚  - Chunk Processing  â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â”‚ HTTP
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        DATA LAYER                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Chroma Vector Store  â”‚         â”‚  Redis Cache          â”‚   â”‚
â”‚  â”‚  (Port 8000)          â”‚         â”‚  (Port 6379)          â”‚   â”‚
â”‚  â”‚  - FastAPI Service    â”‚         â”‚  - Job Queue          â”‚   â”‚
â”‚  â”‚  - DuckDB + Parquet   â”‚         â”‚  - Caching            â”‚   â”‚
â”‚  â”‚  - Per-User Collectionsâ”‚         â”‚  - Session State      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Metadata Store (storage.json)                          â”‚ â”‚
â”‚  â”‚  - Document Metadata                                     â”‚ â”‚
â”‚  â”‚  - Chunk References                                       â”‚ â”‚
â”‚  â”‚  - Knowledge Graph                                       â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â”‚ API Calls
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      EXTERNAL SERVICES                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                     â”‚
â”‚  â”‚  OpenAI API          â”‚                                     â”‚
â”‚  â”‚  - Embeddings        â”‚                                     â”‚
â”‚  â”‚  - Chat Completions  â”‚                                     â”‚
â”‚  â”‚  - Whisper (Audio)   â”‚                                     â”‚
â”‚  â”‚  - Vision API (Images)â”‚                                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Read this:**

"The system follows a microservices architecture. At the front, we have a React application built with Vite. It communicates with a Node.js Express backend via REST API. The backend handles HTTP requests, manages file uploads, and processes queries. But here's the key design decision - heavy processing happens asynchronously through a worker service."

**Continue:**

"When a file is uploaded, the backend doesn't process it immediately. Instead, it stores metadata and enqueues a job to a Redis queue using BullMQ. A separate worker service consumes these jobs. This separation allows the API to remain responsive while processing happens in the background. Multiple workers can run in parallel, providing horizontal scalability."

**Continue:**

"The worker communicates with a Chroma service - a Python FastAPI microservice that wraps ChromaDB, our vector database. Chroma handles all vector operations: storing embeddings, performing similarity searches, and managing per-user collections. This separation allows us to scale the vector database independently from the API."

**Continue:**

"External services include OpenAI for embeddings, chat completions, audio transcription via Whisper, and image captioning via Vision API. All of these are called asynchronously to avoid blocking the main application flow."

#### Part B: Multi-Modal Ingestion Pipeline

**Read this:**

"Let me dive deeper into the ingestion pipeline, as this is where much of the complexity lies."

**Show this diagram:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    INGESTION PIPELINE                           â”‚
â”‚                                                                 â”‚
â”‚  User Upload                                                    â”‚
â”‚      â”‚                                                          â”‚
â”‚      â–¼                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                               â”‚
â”‚  â”‚  Backend API â”‚                                               â”‚
â”‚  â”‚  - Store     â”‚                                               â”‚
â”‚  â”‚    metadata  â”‚                                               â”‚
â”‚  â”‚  - Enqueue   â”‚                                               â”‚
â”‚  â”‚    job       â”‚                                               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                               â”‚
â”‚         â”‚                                                        â”‚
â”‚         â”‚ BullMQ Job                                            â”‚
â”‚         â–¼                                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                               â”‚
â”‚  â”‚   Worker    â”‚                                               â”‚
â”‚  â”‚   Service   â”‚                                               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                               â”‚
â”‚         â”‚                                                        â”‚
â”‚         â”œâ”€â”€â–º PDF â†’ pdf-parse â†’ Text                             â”‚
â”‚         â”‚                                                        â”‚
â”‚         â”œâ”€â”€â–º Audio â†’ Whisper API â†’ Transcript                   â”‚
â”‚         â”‚                                                        â”‚
â”‚         â”œâ”€â”€â–º Image â†’ OpenAI Vision API â†’ Caption                â”‚
â”‚         â”‚                                                        â”‚
â”‚         â”œâ”€â”€â–º URL â†’ Cheerio â†’ Scraped Text                       â”‚
â”‚         â”‚                                                        â”‚
â”‚         â””â”€â”€â–º Text â†’ Direct                                      â”‚
â”‚         â”‚                                                        â”‚
â”‚         â–¼                                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                               â”‚
â”‚  â”‚   Chunking  â”‚                                               â”‚
â”‚  â”‚  (~400 wordsâ”‚                                               â”‚
â”‚  â”‚   15% overlap)â”‚                                             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                               â”‚
â”‚         â”‚                                                        â”‚
â”‚         â–¼                                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                               â”‚
â”‚  â”‚  Embedding  â”‚                                               â”‚
â”‚  â”‚  Generation â”‚                                               â”‚
â”‚  â”‚  (OpenAI)   â”‚                                               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                               â”‚
â”‚         â”‚                                                        â”‚
â”‚         â–¼                                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                               â”‚
â”‚  â”‚   Chroma    â”‚                                               â”‚
â”‚  â”‚   Storage   â”‚                                               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Read this:**

"When content arrives, it goes through a transformation pipeline. PDFs are parsed using pdf-parse, extracting text while preserving structure. Audio files are sent to OpenAI's Whisper API for transcription - the system handles various formats like MP3, M4A, WAV, and more. Images go to OpenAI's Vision API using GPT-4o-mini for detailed captioning. URLs are scraped server-side using cheerio, extracting text from paragraph tags while handling authentication-required pages gracefully. Plain text is used directly."

**Continue:**

"Once we have text, it's chunked into segments of approximately 400 words with a 15% overlap. This chunking strategy balances several concerns. Smaller chunks provide more precise retrieval, but lose context. Larger chunks preserve context but may include irrelevant information. The 400-word size, combined with 15% overlap, ensures we capture complete thoughts while maintaining retrieval precision. The overlap prevents important information from being split across chunk boundaries."

**Continue:**

"Each chunk is then converted to a 1536-dimensional vector using OpenAI's text-embedding-3-small model. These embeddings capture semantic meaning, allowing the system to find conceptually similar content even when exact keywords don't match."

**Show this diagram:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CHUNKING STRATEGY                            â”‚
â”‚                                                                 â”‚
â”‚  Original Text:                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  [Paragraph 1] [Paragraph 2] [Paragraph 3] [Paragraph 4]â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                 â”‚
â”‚  Chunked (400 words, 15% overlap):                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                          â”‚
â”‚  â”‚  Chunk 1         â”‚  (words 1-400)                          â”‚
â”‚  â”‚  [Para 1-2]      â”‚                                          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                          â”‚
â”‚         â”‚ 15% overlap                                           â”‚
â”‚         â–¼                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                          â”‚
â”‚  â”‚  Chunk 2         â”‚  (words 340-740)                         â”‚
â”‚  â”‚  [Para 2-3]      â”‚                                          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                          â”‚
â”‚         â”‚ 15% overlap                                           â”‚
â”‚         â–¼                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                          â”‚
â”‚  â”‚  Chunk 3         â”‚  (words 680-1080)                        â”‚
â”‚  â”‚  [Para 3-4]      â”‚                                          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                          â”‚
â”‚                                                                 â”‚
â”‚  Each chunk â†’ Embedding (1536 dimensions) â†’ Chroma Vector Storeâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Continue:**

"The vectors are stored in Chroma along with metadata - document ID, user ID, source type, and timestamps. This metadata enables filtering and hybrid scoring later."

**Continue:**

"The system also extracts entities from the combined text using an LLM, building a simple knowledge graph. While not heavily used in the current version, this provides a foundation for future graph-based query capabilities."

#### Part C: Query Processing and Retrieval

**Read this:**

"The query processing system is where the real intelligence happens. It's not just a simple vector search - it's a sophisticated hybrid retrieval system."

**Show this diagram:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    QUERY PROCESSING FLOW                        â”‚
â”‚                                                                 â”‚
â”‚  User Query: "What did I learn last week?"                     â”‚
â”‚      â”‚                                                          â”‚
â”‚      â–¼                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                           â”‚
â”‚  â”‚  Parse Query    â”‚                                           â”‚
â”‚  â”‚  - Extract time â”‚                                           â”‚
â”‚  â”‚  - Keywords     â”‚                                           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                           â”‚
â”‚         â”‚                                                       â”‚
â”‚         â–¼                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                           â”‚
â”‚  â”‚  Generate Query â”‚                                           â”‚
â”‚  â”‚  Embedding      â”‚                                           â”‚
â”‚  â”‚  (OpenAI)       â”‚                                           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                           â”‚
â”‚         â”‚                                                       â”‚
â”‚         â–¼                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                           â”‚
â”‚  â”‚  Vector Search  â”‚                                           â”‚
â”‚  â”‚  (Chroma)       â”‚                                           â”‚
â”‚  â”‚  - Top 15       â”‚                                           â”‚
â”‚  â”‚    candidates   â”‚                                           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                           â”‚
â”‚         â”‚                                                       â”‚
â”‚         â–¼                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                           â”‚
â”‚  â”‚  Temporal       â”‚                                           â”‚
â”‚  â”‚  Filtering      â”‚                                           â”‚
â”‚  â”‚  (chrono-node)  â”‚                                           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                           â”‚
â”‚         â”‚                                                       â”‚
â”‚         â–¼                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                           â”‚
â”‚  â”‚  Hybrid Scoring â”‚                                           â”‚
â”‚  â”‚  - 85% Semantic â”‚                                           â”‚
â”‚  â”‚  - 15% Keyword  â”‚                                           â”‚
â”‚  â”‚  - 10% Recency  â”‚                                           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                           â”‚
â”‚         â”‚                                                       â”‚
â”‚         â–¼                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                           â”‚
â”‚  â”‚  Top-K Chunks   â”‚                                           â”‚
â”‚  â”‚  (k=5)          â”‚                                           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                           â”‚
â”‚         â”‚                                                       â”‚
â”‚         â–¼                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                           â”‚
â”‚  â”‚  LLM Generation â”‚                                           â”‚
â”‚  â”‚  (GPT-4o-mini)  â”‚                                           â”‚
â”‚  â”‚  - Context      â”‚                                           â”‚
â”‚  â”‚  - Stream Answerâ”‚                                           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                           â”‚
â”‚         â”‚                                                       â”‚
â”‚         â–¼                                                       â”‚
â”‚  Stream Answer + Sources (SSE)                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Explain:**

"When a query comes in, several things happen in parallel. First, the query text is parsed to extract temporal constraints using chrono-node. Phrases like 'last week' or 'in December' are converted to date ranges. The query is also converted to an embedding using the same model used for documents."

**Continue:**

"The system then queries Chroma for candidate chunks. I retrieve 3 times the requested number - so if the user wants 5 results, I get 15 candidates. This over-retrieval is important because we'll filter and re-score these candidates."

**Continue:**

"Here's where the hybrid scoring comes in. Each candidate gets three scores."

**Show this diagram:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    HYBRID SCORING FORMULA                       â”‚
â”‚                                                                 â”‚
â”‚  Final Score = 0.85 Ã— Semantic + 0.15 Ã— Keyword + 0.10 Ã— Recencyâ”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Semantic Score  â”‚  â”‚  Keyword Score   â”‚  â”‚ Recency Boostâ”‚ â”‚
â”‚  â”‚  (85% weight)    â”‚  â”‚  (15% weight)    â”‚  â”‚ (10% weight) â”‚ â”‚
â”‚  â”‚                  â”‚  â”‚                  â”‚  â”‚              â”‚ â”‚
â”‚  â”‚  Cosine          â”‚  â”‚  Substring       â”‚  â”‚ Exponential  â”‚ â”‚
â”‚  â”‚  Similarity      â”‚  â”‚  Frequency       â”‚  â”‚ Decay        â”‚ â”‚
â”‚  â”‚  from Chroma     â”‚  â”‚  Matching        â”‚  â”‚ Function     â”‚ â”‚
â”‚  â”‚                  â”‚  â”‚                  â”‚  â”‚              â”‚ â”‚
â”‚  â”‚  Range: 0.0-1.0  â”‚  â”‚  Normalized      â”‚  â”‚ Favors last  â”‚ â”‚
â”‚  â”‚                  â”‚  â”‚                  â”‚  â”‚ 30 days     â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚         â”‚                       â”‚                    â”‚          â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                            â”‚                                    â”‚
â”‚                            â–¼                                    â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”‚
â”‚                    â”‚ Final Score  â”‚                            â”‚
â”‚                    â”‚ (Sorted)     â”‚                            â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Continue:**

"First, semantic similarity from the vector search - this captures conceptual relevance. Second, keyword matching - a simple substring frequency score that catches exact matches. Third, recency boost - an exponential decay function that favors recent documents. The final score combines these: 85% semantic, 15% keyword, and 10% recency boost. This hybrid approach gives us the best of both worlds - semantic understanding plus traditional search capabilities."

**Continue:**

"Before final scoring, temporal filters are applied. If the user asked about 'last week', only chunks from that time period are considered. This filtering happens early to improve efficiency."

**Continue:**

"The top-k chunks are then sent to GPT-4o-mini as context. The prompt instructs the LLM to answer only using the provided context, ensuring accuracy and preventing hallucination. The LLM streams the answer token-by-token using Server-Sent Events (SSE), providing immediate feedback to users. The system also returns source citations with document metadata - including filenames, URLs, and source types - which are deduplicated so each unique document appears only once."

#### Part D: Data Storage Architecture

**Read this:**

"Data storage is split across multiple systems, each optimized for its purpose."

**Show this diagram:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DATA STORAGE ARCHITECTURE                    â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Chroma Vector Store (Per-User Collections)                â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ â”‚
â”‚  â”‚  â”‚ User 1       â”‚  â”‚ User 2       â”‚  â”‚ User 3       â”‚  â”‚ â”‚
â”‚  â”‚  â”‚ Collection   â”‚  â”‚ Collection   â”‚  â”‚ Collection   â”‚  â”‚ â”‚
â”‚  â”‚  â”‚              â”‚  â”‚              â”‚  â”‚              â”‚  â”‚ â”‚
â”‚  â”‚  â”‚ Vectors:     â”‚  â”‚ Vectors:     â”‚  â”‚ Vectors:     â”‚  â”‚ â”‚
â”‚  â”‚  â”‚ - Embeddings â”‚  â”‚ - Embeddings â”‚  â”‚ - Embeddings â”‚  â”‚ â”‚
â”‚  â”‚  â”‚ - Metadata   â”‚  â”‚ - Metadata   â”‚  â”‚ - Metadata   â”‚  â”‚ â”‚
â”‚  â”‚  â”‚ - Documents   â”‚  â”‚ - Documents   â”‚  â”‚ - Documents   â”‚  â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚
â”‚  â”‚                                                           â”‚ â”‚
â”‚  â”‚  Storage: DuckDB (metadata) + Parquet (vectors)          â”‚ â”‚
â”‚  â”‚  Index: HNSW (Hierarchical Navigable Small World)        â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Metadata Store (storage.json)                           â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ â”‚
â”‚  â”‚  â”‚ docs[]       â”‚  â”‚ chunks[]     â”‚  â”‚ graph{}      â”‚  â”‚ â”‚
â”‚  â”‚  â”‚ - docId      â”‚  â”‚ - chunkId    â”‚  â”‚ - nodes[]    â”‚  â”‚ â”‚
â”‚  â”‚  â”‚ - userId     â”‚  â”‚ - docId      â”‚  â”‚ - edges[]    â”‚  â”‚ â”‚
â”‚  â”‚  â”‚ - filename   â”‚  â”‚ - text       â”‚  â”‚              â”‚  â”‚ â”‚
â”‚  â”‚  â”‚ - sourceType â”‚  â”‚ - createdAt  â”‚  â”‚              â”‚  â”‚ â”‚
â”‚  â”‚  â”‚ - uploadedAt â”‚  â”‚              â”‚  â”‚              â”‚  â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚
â”‚  â”‚                                                           â”‚ â”‚
â”‚  â”‚  Production: Would migrate to PostgreSQL + pgvector      â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Redis (Job Queue + Cache)                                â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚ â”‚
â”‚  â”‚  â”‚ BullMQ Queue â”‚  â”‚ Cache Layer  â”‚                     â”‚ â”‚
â”‚  â”‚  â”‚ - upload_fileâ”‚  â”‚ - Frequent  â”‚                     â”‚ â”‚
â”‚  â”‚  â”‚ - inline_textâ”‚  â”‚   queries   â”‚                     â”‚ â”‚
â”‚  â”‚  â”‚ - ingest_url â”‚  â”‚ - Session   â”‚                     â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Explain:**

"Vector embeddings are stored in Chroma, which uses DuckDB for metadata and Parquet files for vector storage. Each user has their own collection, providing data isolation. This per-user collection design enables horizontal scaling - we can shard collections across multiple Chroma instances as needed. The HNSW index provides fast approximate nearest neighbor search, which is crucial for good performance at scale."

**Continue:**

"Metadata is currently stored in a JSON file called storage.json. This includes document metadata, chunk references, and the knowledge graph. I chose this for the prototype because it requires no database setup and is easy to understand. However, I'm fully aware this isn't production-ready. In a production system, I would migrate to PostgreSQL with pgvector extension, or MongoDB with vector search capabilities. The JSON file approach trades operational simplicity for scalability and concurrency safety."

**Continue:**

"File uploads are stored on the filesystem in an uploads directory. In production, these would go to object storage like AWS S3 or similar. Redis serves dual purposes - as a job queue via BullMQ, and as a caching layer for frequently accessed data."

---

### SECTION 4: KEY ARCHITECTURAL DECISIONS & TRADE-OFFS (2-3 minutes)

**Read this:**

"Now let me walk through the key architectural decisions I made and the trade-offs involved. These decisions shape the entire system, and understanding them is crucial."

#### Decision 1: Chroma vs Pinecone

**Read this:**

"My first major decision was choosing a vector database. I evaluated Chroma and Pinecone, two popular options."

**Show this comparison diagram:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              CHROMA vs PINECONE COMPARISON                      â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚  CHROMA (Chosen)      â”‚  â”‚  PINECONE            â”‚          â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤          â”‚
â”‚  â”‚ âœ… Open Source        â”‚  â”‚ âœ… Managed Service   â”‚          â”‚
â”‚  â”‚ âœ… Self-hostable      â”‚  â”‚ âœ… Auto-scaling      â”‚          â”‚
â”‚  â”‚ âœ… No vendor lock-in  â”‚  â”‚ âœ… Production-ready  â”‚          â”‚
â”‚  â”‚ âœ… No per-query cost  â”‚  â”‚ âœ… Monitoring        â”‚          â”‚
â”‚  â”‚ âœ… Local-first        â”‚  â”‚ âœ… Optimization     â”‚          â”‚
â”‚  â”‚                      â”‚  â”‚                      â”‚          â”‚
â”‚  â”‚ âŒ Less managed       â”‚  â”‚ âŒ Vendor lock-in    â”‚          â”‚
â”‚  â”‚ âŒ Smaller community  â”‚  â”‚ âŒ Higher cost       â”‚          â”‚
â”‚  â”‚ âŒ More setup         â”‚  â”‚ âŒ Less control      â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                                                 â”‚
â”‚  Trade-off: Flexibility & Cost Control vs Managed Convenience  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Explain:**

"I chose Chroma for several reasons. First, it's open source and self-hostable, which means no vendor lock-in and no per-query costs. Second, it's excellent for prototyping and local-first deployments. Third, it uses DuckDB and Parquet, which are well-understood technologies. However, this choice comes with trade-offs. Pinecone is a managed service with better scaling characteristics and production reliability. It handles infrastructure, monitoring, and optimization automatically. But it's more expensive and creates vendor dependency."

**Continue:**

"The trade-off here is flexibility and cost control versus managed convenience. For a prototype and learning project, Chroma was the right choice. For a production system at scale, I might reconsider Pinecone or other managed options."

#### Decision 2: JSON File vs Database

**Read this:**

"Another significant decision was how to store metadata. I chose a JSON file for the prototype, but this is clearly not production-ready."

**Show this comparison:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          JSON FILE vs DATABASE COMPARISON                       â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚  storage.json        â”‚  â”‚  PostgreSQL          â”‚          â”‚
â”‚  â”‚  (Current)           â”‚  â”‚  (Production)        â”‚          â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤          â”‚
â”‚  â”‚ âœ… No setup needed   â”‚  â”‚ âœ… ACID transactions â”‚          â”‚
â”‚  â”‚ âœ… Easy to debug     â”‚  â”‚ âœ… Concurrent safe   â”‚          â”‚
â”‚  â”‚ âœ… Simple prototype  â”‚  â”‚ âœ… Scalable          â”‚          â”‚
â”‚  â”‚ âœ… Human readable    â”‚  â”‚ âœ… Better performanceâ”‚          â”‚
â”‚  â”‚                      â”‚  â”‚ âœ… Backup/recovery  â”‚          â”‚
â”‚  â”‚ âŒ Not concurrent    â”‚  â”‚                      â”‚          â”‚
â”‚  â”‚ âŒ Limited scale     â”‚  â”‚ âŒ More complex      â”‚          â”‚
â”‚  â”‚ âŒ No transactions   â”‚  â”‚ âŒ Requires setup    â”‚          â”‚
â”‚  â”‚ âŒ Data loss risk    â”‚  â”‚                      â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                                                 â”‚
â”‚  Migration Path: Clear mapping to database tables              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Explain:**

"The JSON file approach has clear benefits: no database setup required, easy to inspect and debug, perfect for prototyping. It makes the system immediately runnable without external dependencies. However, it has serious limitations: no concurrent write safety, limited scalability, no transactions, and potential data loss risks."

**Continue:**

"The trade-off is development speed and simplicity versus production readiness. For this project, I prioritized getting a working system quickly. In production, I would absolutely migrate to PostgreSQL with pgvector, which provides ACID transactions, concurrent access, and better performance. The migration path is clear - the data structure in storage.json maps directly to database tables."

#### Decision 3: OpenAI APIs vs Local Models

**Read this:**

"I chose to use OpenAI's APIs for embeddings and chat, rather than running local models. This was a deliberate trade-off."

**Explain:**

"OpenAI's APIs provide excellent quality. The text-embedding-3-small model produces high-quality embeddings, and GPT-4o-mini provides reliable chat completions. Integration is straightforward, and I don't need to manage model infrastructure. However, this creates external dependencies, ongoing API costs, and privacy concerns since data leaves my system."

**Continue:**

"The alternative would be local models - using sentence-transformers for embeddings and something like Ollama for chat. This would provide better privacy, no API costs, and offline capability. But it requires more infrastructure, lower quality embeddings potentially, and more maintenance overhead."

**Continue:**

"I chose OpenAI for quality and simplicity, accepting the trade-offs. In a production system, I might offer both options - allowing users to choose between cloud APIs for convenience or local models for privacy."

#### Decision 4: Fixed Chunking vs Semantic Chunking

**Read this:**

"For text chunking, I implemented fixed-size chunking with sentence awareness, rather than semantic chunking."

**Explain:**

"Fixed-size chunking is simple and predictable. I know exactly how many chunks a document will produce, making it easier to reason about storage and retrieval. The 400-word size with 15% overlap works well for most content. Sentence-aware splitting prevents mid-sentence breaks, maintaining readability."

**Continue:**

"Semantic chunking would be more intelligent - splitting at natural topic boundaries rather than arbitrary word counts. This could improve retrieval quality by keeping related concepts together. However, it's more complex to implement, less predictable, and requires additional processing."

**Continue:**

"I chose fixed chunking for simplicity and predictability. The overlap helps mitigate the downsides. In production, I might experiment with semantic chunking for certain document types while keeping fixed chunking as the default."

#### Decision 5: Async Processing Architecture

**Read this:**

"Perhaps the most important architectural decision was using async processing with a job queue."

**Explain:**

"I implemented BullMQ with Redis for job queuing. When files are uploaded, jobs are enqueued immediately, and the API responds right away. Workers process jobs asynchronously. This provides several benefits: the API stays responsive, jobs can be retried on failure, and workers can scale horizontally. Multiple workers can process jobs in parallel, and we can add more workers as load increases."

**Continue:**

"The alternative would be synchronous processing - processing uploads immediately in the API request. This would be simpler but would block the API, leading to poor user experience for large files. Users would have to wait for PDF parsing, embedding generation, and vector storage to complete before getting a response."

**Continue:**

"The trade-off is complexity versus user experience. The async approach adds Redis, BullMQ, and worker services, but provides a much better user experience. This was absolutely the right choice, and I'd make it again in production."

---

### SECTION 5: SCALABILITY & FUTURE ENHANCEMENTS (1 minute)

**Read this:**

"Let me briefly discuss scalability and future enhancements."

**Explain:**

"The architecture is designed for horizontal scaling. The API is stateless and can run multiple instances behind a load balancer. Workers can scale independently based on queue depth. Chroma collections can be sharded across instances. Redis can be clustered for high availability."

**Continue:**

"For future enhancements, I would prioritize authentication and authorization - currently the system uses a simple user ID header. I'd implement JWT-based authentication with proper session management. I'd migrate metadata storage to PostgreSQL for production reliability. I'd add advanced search features like faceted search and saved searches. Multi-language support would be valuable. And I'd consider adding a local-first mode that can run entirely offline with local models."

**Continue:**

"The current architecture provides a solid foundation for these enhancements without requiring major refactoring."

---

### SECTION 6: CONCLUSION (30-45 seconds)

**Read this:**

"In summary, SecondBrain demonstrates a modern RAG architecture with multi-modal ingestion, hybrid retrieval, and async processing. The key architectural decisions I made prioritize user experience and development speed while maintaining a clear path to production scalability."

**Continue:**

"Each trade-off was made deliberately, balancing simplicity with functionality, and prototype speed with production readiness. The system shows how modern AI capabilities can be integrated into a practical knowledge management tool."

**Continue:**

"Thank you for watching this walkthrough. I hope it gave you insight into both what the system does and why it's built this way. If you have questions about any of the architectural decisions or want to see specific code, feel free to check out the repository. Thanks again!"

---

## RECORDING TIPS

### Before Recording:

1. **Test Everything**: Make sure all features work before recording
2. **Prepare Sample Files**: Have a PDF, image, text snippet, and URL ready
3. **Clear Browser**: Use a clean browser session or incognito mode
4. **Check Audio**: Test your microphone and ensure quiet environment
5. **Screen Setup**: Close unnecessary applications, use a clean desktop
6. **Practice Run**: Do a practice run to get comfortable with the script

### During Recording:

1. **Speak Clearly**: Read naturally, don't rush
2. **Show, Don't Just Tell**: Actually demonstrate features as you describe them
3. **Pause for Effect**: Don't be afraid to pause briefly between sections
4. **Cursor Movement**: Move your cursor deliberately to guide viewer attention
5. **Stay Calm**: If you make a mistake, pause and continue - you can edit later

### Post-Production:

1. **Edit Out Mistakes**: Remove long pauses and mistakes
2. **Add Title Slide**: Include project name and your name
3. **Add Captions**: If possible, add subtitles for accessibility
4. **Trim Dead Time**: Remove unnecessary waiting or loading screens
5. **Add Transitions**: Smooth transitions between sections
6. **Final Check**: Watch the full video before submitting

---

## TIMING BREAKDOWN

- **Introduction**: 30-45 seconds
- **Application Demo**: 3-4 minutes
- **Architecture Walkthrough**: 4-5 minutes  
- **Trade-offs Discussion**: 2-3 minutes
- **Scalability & Future**: 1 minute
- **Conclusion**: 30-45 seconds

**Total: 11-14 minutes** (can be trimmed to 8-10 minutes if needed)

---

## KEY POINTS TO REMEMBER

1. **Be Natural**: Read the script naturally, don't sound robotic
2. **Demonstrate**: Actually use the application, don't just describe it
3. **Explain Reasoning**: For each decision, explain WHY, not just WHAT
4. **Be Honest**: Acknowledge limitations and trade-offs openly
5. **Show Confidence**: You made deliberate choices - own them
6. **Stay Focused**: Keep to the main points, don't get sidetracked

---

Good luck with your recording! This script provides everything you need to create a comprehensive and professional walkthrough video.
